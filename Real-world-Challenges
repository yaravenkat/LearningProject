# Real-world Challenges & Resolutions — Java, Spring, Spring Boot, Spring Security, JPA, API Gateway & Microservices

This file collects real production problems I've seen (or that teams commonly face), how we diagnosed them, detailed fixes (code/config examples), validation steps, and lessons learned. Each item is a self-contained case study with actionable guidance you can apply immediately.

Contents
- Summary checklist
- Observability / tooling used
- Case studies
  1. Slow endpoints due to N+1 JPA queries
  2. High latency & thread-pool exhaustion (blocking I/O in reactive code)
  3. Memory leak caused by improper caching / retention
  4. Transaction anomalies & optimistic locking failures
  5. Auth token propagation through API Gateway → services
  6. Thundering retries + failed circuit breakers
  7. DB connection pool exhaustion / slow prepared-statement issues
  8. API Gateway routing, rate limiting and path rewriting problems
  9. Service discovery flapping and startup storms
  10. Schema migration & versioning in microservices (backwards compatibility)
- Cross-cutting mitigations (CI, testing, deployment)
- Monitoring / SLOs / incident runbook checklist

---

Summary checklist (quick reference)
- Log SQL with bind parameters in non-production; enable `spring.jpa.show-sql=false` in prod (use APM instead).
- Use DTO projections, fetch-join, or entity graphs to avoid N+1.
- Use HikariCP and tune maxPoolSize; monitor active/max connections.
- Keep transactions short; use readOnly=true for queries.
- Use Resilience4j (circuit-breaker + retry + bulkhead) with proper backoffs.
- Propagate auth tokens (JWT/OAuth2) in gateway filters and Feign/WebClient interceptors.
- Use bounded thread pools for blocking tasks; avoid blocking on Netty/event-loop threads.
- Use Flyway for DB migrations and maintain backward-compatible schema changes for rolling deployments.
- Centralized logging, distributed tracing (OpenTelemetry/Jaeger/Zipkin), and metrics (Prometheus + Grafana).

---

Observability & tooling (frequently used)
- JVM: jcmd, jmap, jstack, Java Flight Recorder (JFR), VisualVM, MAT (Memory Analyzer)
- App/server metrics: Micrometer → Prometheus → Grafana
- Tracing: OpenTelemetry, Jaeger, Zipkin, Spring Cloud Sleuth (legacy)
- Logs: Elastic Stack (ELK/EFK) or Loki
- DB analysis: EXPLAIN/EXPLAIN ANALYZE, psql/mysql client, slow query logs
- Load testing: Gatling, k6, JMeter
- Local debugging of SQL/transactions: p6spy, datasource-proxy
- CI/CD: GitHub Actions / Jenkins / GitLab CI + Canary / Blue-Green deployments

---

Case study 1 — N+1 queries in JPA causing slow endpoints
Problem
- Endpoint that returns an Order with its Items and Customer takes seconds and CPU spikes when load increases.

Symptoms
- Many SQL `select` statements in logs (1 select for orders + N selects for items).
- High response time, high DB CPU, I/O.

Root cause
- JPA default lazy/eager fetch mismatch; code iterated over `order.getItems()` while entity manager closed (or triggered lazy loads repeatedly).
- Use of entity-to-JSON serialization (Jackson) triggered traversal of lazy associations.

Diagnosis
- Enable SQL logging in dev/trace (or use p6spy) and check number of queries per request.
- Capture a request in dev and inspect logs: repeated `select` on `order_item` per order.

Fixes (stepwise)
1. Immediate quick fix: fetch join in repository for the specific endpoint:
```java
@Query("select o from Order o left join fetch o.items where o.id = :id")
Optional<Order> findByIdWithItems(@Param("id") Long id);
```
2. Prefer DTO projection to avoid returning entities and control exactly what is fetched:
```java
public record OrderDto(Long id, String customerName, List<ItemDto> items) {}
@Query("select new com.example.dto.OrderDto(o.id, c.name, ... ) from Order o join o.customer c where o.id = :id")
OrderDto findOrderDtoById(Long id);
```
3. Use `@EntityGraph` for optional fetch join:
```java
@EntityGraph(attributePaths = {"items", "customer"})
Optional<Order> findById(Long id);
```

Validation & verification
- Re-run load test and confirm SQL count reduced to 1 (or minimal) per request.
- Check metrics: p99 latency reduced and DB CPU lower.
- Add automated integration test asserting repository method uses JOIN (harder — assert behavior via logs or counter).

Lessons learned
- Never rely on default fetch patterns for API endpoints; prefer DTOs for external boundaries.
- Keep entity graphs under control; large graphs lead to massive data fetches.

---

Case study 2 — Thread-pool exhaustion & blocking I/O in reactive endpoints
Problem
- WebFlux service with DB calls and 3rd-party HTTP calls started to have high latencies and eventual 503 errors.

Symptoms
- Netty event-loop threads blocked (observed via thread dumps or JFR).
- High percent of threads in WAIT or BLOCKED states.
- Outbound connection timeouts and request queue building on client side.

Root cause
- Blocking JDBC calls (or blocking processing) executed directly on Netty event-loop (default reactive thread pool), causing event-loop starvation.

Diagnosis
- Thread dump: Netty event-loop threads show stack frames waiting on JDBC call.
- Tracing: long spans on DB or 3rd-party calls.

Fixes
1. Offload blocking work to a bounded, dedicated scheduler:
```java
@Bean
public Scheduler jdbcScheduler() {
    return Schedulers.boundedElastic(); // or custom thread pool
}

// In reactive handler:
Mono.fromCallable(() -> blockingRepo.find(...))
    .subscribeOn(jdbcScheduler())
    .map(this::toDto);
```
2. Preferred: adopt R2DBC (reactive DB driver) or use WebClient for HTTP calls (non-blocking).
3. Limit concurrency: use Reactor's `.limitRate()` or semaphore/bulkhead pattern.

Validation
- Stress test with same load; verify event-loop thread latencies normalized; check reactor/operator metrics.
- Confirm overall p95/p99 latency reduced.

Lessons learned
- Do not block reactive thread-pools. Either use fully non-blocking stack (WebFlux + R2DBC) or carefully isolate blocking calls on bounded pools.

---

Case study 3 — Memory leak due to improper caching
Problem
- Heap usage grows steadily and eventually OutOfMemoryError (OOM).

Symptoms
- Heap size increases slowly over days; GC spends more time; memory not reclaimed.
- Application restarts sometimes recover.

Root cause
- Unbounded cache (ConcurrentHashMap) used to store large objects keyed by long-lived keys; entries never evicted.
- Using ThreadLocal that isn’t cleared in pooled threads.

Diagnosis
- Generate heap dump at peak memory; analyze with MAT (Memory Analyzer) — found large retainers from `com.example.cache.LocalCache` or `ThreadLocal` objects holding references.
- GC logs show long-tenured objects.

Fixes
1. Replace ad-hoc cache with Caffeine/Guava with maxSize and TTL:
```java
Caffeine.newBuilder()
    .maximumSize(10_000)
    .expireAfterWrite(Duration.ofMinutes(30))
    .build();
```
2. Use Spring Cache abstraction with proper cache manager (CaffeineCacheManager) and `@Cacheable` with sensible TTL/size.
3. Clear ThreadLocals in finally blocks and for pooled/executor thread use utilities to clean-up.

Validation
- After deployment, monitor heap growth (Micrometer JVM memory metrics) and GC pause times. Validate no OOM over test duration.

Lessons learned
- Never invent your own long-lived caches without eviction policies. Use battle-tested libraries and set measurable limits.

---

Case study 4 — Transaction anomalies & optimistic locking exceptions
Problem
- Users occasionally reported lost updates or `OptimisticLockException` when updating same resource concurrently.

Symptoms
- `javax.persistence.OptimisticLockException` thrown; some updates failed; others silently overwrote.

Root cause
- Concurrent updates without proper concurrency control and using detached entities modified outside transactional boundaries.

Diagnosis
- Review code: UI read -> edit -> submit uses stale DTO; update does not verify version or uses detached merge operations.
- DB access logs show concurrent UPDATE statements.

Fixes
1. Add a `@Version` field to entity for optimistic locking:
```java
@Version
private Long version;
```
2. Use `@Transactional` on service update method and re-read entity within transaction:
```java
@Transactional
public void updateOrder(Long id, UpdateDto dto) {
    Order order = repo.findById(id).orElseThrow(...);
    order.apply(dto);
    // flush happens at commit; version checked
}
```
3. Provide user-friendly handling: catch `OptimisticLockException` and return 409 Conflict with message to refresh data.
4. For workflows requiring multiple steps, consider pessimistic locking (`SELECT ... FOR UPDATE`) or implement application-level reconciliation strategies (last-writer-wins vs merge UI).

Validation
- Simulate concurrent updates in tests; verify 409 responses and safe behavior.

Lessons learned
- Prefer optimistic locking for web apps with relatively low contention; always handle lock exceptions gracefully and inform users.

---

Case study 5 — Authentication token propagation across API Gateway → services
Problem
- User is authenticated at API Gateway (JWT issued by auth service) but downstream services don't receive user context; calls fail or lack audit info.

Symptoms
- Downstream services receive no Authorization header; logs show anonymous requests; auditing fails.

Root cause
- API Gateway (Spring Cloud Gateway) not forwarding headers/or rewriting them, or Feign/WebClient clients not propagating header to downstream calls.

Diagnosis
- Capture headers at gateway and service ingress (via logging or tracing) to confirm header loss.
- Check gateway route config and filters.

Fixes
1. Configure Gateway to forward Authorization header and any custom headers:
```yaml
spring:
  cloud:
    gateway:
      routes:
      - id: users
        uri: lb://USER-SERVICE
        predicates:
        - Path=/api/users/**
        filters:
        - RemoveRequestHeader=Cookie
        - AddRequestHeader=X-Request-Id, #{T(java.util.UUID).randomUUID().toString()}
        - PreserveHostHeader=true
```
By default Gateway forwards headers, but some global filters might remove them. Use `SetRequestHeader` or `AddRequestHeader` in extreme cases.

2. For OAuth2/JWT introspection or token relay:
- If using opaque tokens: use `OAuth2AuthorizedClient` and propagate tokens using a WebClient `ExchangeFilterFunction` or Feign `RequestInterceptor`.

Example: a WebClient that propagates current JWT:
```java
@Bean
public WebClient webClient(ServerOAuth2AuthorizedClientExchangeFilterFunction oauth2) {
    return WebClient.builder()
        .filter(oauth2)
        .build();
}
```
For manual header copy (in gateway filter):
```java
// GlobalFilter implementation
exchange.getRequest().mutate()
    .header(HttpHeaders.AUTHORIZATION, token);
```

3. For Feign with OAuth2 client:
- Create `RequestInterceptor` to add `Authorization` header from SecurityContext.

Validation
- Perform an end-to-end request; check headers at each service; confirm logs/traces show the authenticated principal.

Lessons learned
- Token relay is essential for audit/security. Prefer centralized token validation (resource server) and propagate only info needed. Be careful with token size & sensitive header exposure.

---

Case study 6 — Thundering retries + circuit breakers causing cascading failures
Problem
- A downstream service becomes slow; clients retry aggressively; combined load causes downstream to crash and cascade.

Symptoms
- Spikes in CPU and latency across services; many retries in logs; circuit breaker states flip to OPEN too late.

Root cause
- Default retry behavior with no backoff and no bulkhead; retry storms combined with synchronous retry amplify load.

Diagnosis
- Trace calls and counts; find many retries within short windows; monitor circuit-breaker metrics.

Fixes
1. Use Resilience4j with backoff and jitter:
```java
@CircuitBreaker(name="users", fallbackMethod="userFallback")
@Retry(name="users", fallbackMethod="userFallback")
public UserDto getUser(Long id) { ... }
```
Configure exponential backoff + jitter in config:
```yaml
resilience4j.retry:
  instances:
    users:
      maxAttempts: 3
      waitDuration: 500ms
      exponentialBackoff:
        multiplier: 2.0
```
2. Add Bulkhead (semaphore) to limit concurrent calls per service:
```java
@Bulkhead(name="users", type=Bulkhead.Type.SEMAPHORE)
```
3. Use circuit-breaker to fail fast and fallback gracefully (cached response, default).
4. Use rate limiting (API Gateway) to protect downstream services (Redis-based token bucket).

Validation
- Inject faults in downstream (latency) and observe system: circuit breaker should open, retries backing off, fallbacks returning, overall system remains functional.

Lessons learned
- Retries must be combined with backoff+jitter and circuit-breakers; global rate-limits and bulkheads prevent cascading failures.

---

Case study 7 — DB connection pool exhaustion & slow prepared statements
Problem
- Application threads hang waiting for DB connection; requests queue up and p99 latency increases.

Symptoms
- HikariCP activeConnections = maximum, threads blocked on connection acquisition.
- DB slow queries and locks observed.

Root cause
- Long-running transactions holding connections, missing pagination on queries, or connection leaks (connections not closed).
- Inefficient use of `@Transactional` causing external code to hold connections.

Diagnosis
- Monitor HikariCP metrics (active, idle, wait count).
- Thread dump shows blocked threads waiting in `HikariPool.getConnection`.
- Check DB for long-running queries: `SHOW PROCESSLIST` / `pg_stat_activity`.

Fixes
1. Fix slow queries (add index, avoid SELECT * with large scans).
2. Ensure `@Transactional` boundaries small and short; avoid network calls inside transaction (e.g., HTTP calls while holding DB connection).
3. Use connection leak detection temporarily:
```yaml
spring:
  datasource:
    hikari:
      leakDetectionThreshold: 2000 # ms
```
4. Increase pool size only if DB can sustain it; otherwise tune queries or scale DB.
5. Use prepared statements and proper batching for bulk ops.

Validation
- Re-run load tests; Hikari activeConnections stabilize; no blocked threads.

Lessons learned
- DB connections are precious. Keep transactions short, and monitor pool metrics. Use connection leak detection in pre-prod.

---

Case study 8 — API Gateway routing, rate limiting, path rewriting problems
Problem
- After deploying gateway routes, clients hit 404 or credentials lost; rate limits not enforced uniformly.

Symptoms
- Missing headers, wrong upstream path, slow requests due to misconfigured filters.

Root cause
- Route predicates and filters misconfigured (Path/Prefix vs PathRewrite), authentication filters ordered incorrectly; rate limiter not using shared Redis, causing inconsistent limits across instances.

Diagnosis
- Gateway logs and routing table inspection; reproduce request hitting gateway and inspect forwarded request.

Fixes
1. Correct routing and rewrite:
```yaml
spring:
  cloud:
    gateway:
      routes:
      - id: user-service
        uri: lb://USER-SERVICE
        predicates:
        - Path=/api/users/**
        filters:
        - RewritePath=/api/users/(?<remaining>.*), /${remaining}
```
2. Ensure authentication filter runs before rate-limiter or vice versa depending on policy (e.g., rate-limit per user authenticated).
3. Use Redis-backed rate limiter for distributed gateway:
```yaml
spring:
  cloud:
    gateway:
      redis-rate-limiter:
        replenishRate: 10
        burstCapacity: 20
```
4. Add test cases (contract tests) verifying gateway behavior.

Validation
- Integration test through gateway to downstream; verify header preservation, path mapping, and rate-limiting counts.

Lessons learned
- Gateways are powerful; treat them like code: versioned config, tested routes, and guarded deployments (canary) to catch routing regressions.

---

Case study 9 — Service discovery flapping & startup storms
Problem
- Many services register at once, overwhelms discovery server; health checks fail; registration thrash.

Symptoms
- Eureka/Consul server high CPU, many registrations/unregistrations, flapping states.

Root cause
- Rolling restarts/auto-scaling triggered simultaneous registration attempts; discovery server not highly available; TTL/heartbeat mismatches.

Diagnosis
- Check discovery server logs and metrics for registration spike.
- Check network latency to discovery server.

Fixes
1. Make discovery servers highly available (cluster).
2. Add jitter/randomized exponential backoff to service registration and retry logic.
3. Use startup cooldown or readiness probe to delay registration until service fully initialized.
4. For Kubernetes, prefer native service discovery (DNS/Service) over Eureka.

Validation
- Scale up simulated instances; verify discovery server amortizes registration load; no flapping.

Lessons learned
- Synchronous single-point discovery needs HA and client-side backoff to avoid stampedes.

---

Case study 10 — Schema migration & DB versioning in microservices
Problem
- Rolling deploys fail because newer service expects DB column not yet created; older instances still running.

Symptoms
- SQL errors (column not found) on new service; partial functionality broken.

Root cause
- DB migration not backwards-compatible with old code; deploy order wrong.

Diagnosis
- Inspect Flyway/Liquibase migration scripts and deployment sequence.

Fixes
1. Use backward-compatible migrations:
   - Add new column nullable with default; deploy new code that can use new column optionally.
   - In later deploy, backfill data.
   - Finally, remove legacy behavior in a later release.
2. Use migration tooling (Flyway) applied before rolling update; coordinate via CI/CD pipeline.
3. Version your API and DB contracts and avoid schema-breaking changes; use feature toggles.

Validation
- Canary deployments testing new DB schema with old/new services; run integration tests, migration dry-run.

Lessons learned
- DB migrations must be designed for zero-downtime: additive changes, two-step migrations, safe backfills.

---

Cross-cutting mitigations: CI / Tests / Deployment
- Automate migration tests in CI: run Flyway against a copy of production-like DB.
- Contract tests (PACT) to ensure consumer-provider compatibility.
- Canary / Blue-Green deployments to limit blast radius.
- Chaos experiments (chaos engineering) to validate resilience: rate-limits, delays, failures.

---

Monitoring & SLOs (what to track)
- Key metrics: request latency (p50/p90/p99), error rates, throughput, JVM heap/new/old GC pauses, DB query times, connection pool usage, thread pool queue lengths.
- Alerts: elevated error rate, p99 latency crossing threshold, low success rate on health checks, DB connections near capacity.
- Traces: end-to-end request tracing with trace-id propagation to identify bottlenecks.

Incident runbook (quick)
- Triage: check overall health dashboard → error spikes.
- Identify scope: single service, gateway, DB, network.
- Rollback or scale down problematic change (canary/blue-green).
- Apply mitigations: open circuit-breakers, adjust rate-limits, increase replicas.
- Post-incident: root-cause analysis, add monitoring, fix tests/add contract checks.

---

References & further reading
- JFR & jcmd docs for JVM profiling
- HikariCP documentation for pool tuning
- Resilience4j docs for retries/circuit-breakers
- Spring Cloud Gateway docs for routing & filters
- Flyway/Liquibase docs for DB migrations
- Micrometer + OpenTelemetry for metrics and tracing

---

